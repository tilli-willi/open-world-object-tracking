running eval of BURST
+ EXP_DIR=exps
+ PY_ARGS=
+ WANDB_NAME=PROB_V1
+ PY_ARGS=
+ python -u main_open_world.py --predict_custom --burst_subdataset ArgoVerse --burst_subset val --detections_path /home/uig93971/src/Hyp-OW-burst-eval/detections --burst_annot_path /home/uig93971/src/data/TAO/burst_annotations --tao_frames_path /home/uig93971/src/data/TAO/frames --dataset HIERARCHICAL --PREV_INTRODUCED_CLS 80 --CUR_INTRODUCED_CLS 0 --train_set '' --test_set burst_val_test --epochs 191 --lr_drop 35 --model_type hypow --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/hypow_t4_ft_hierarchical_split.pth --eval --wandb_project '' --wandb_name PROB_V1_t1 --wandb_project '' --lr_drop 40 --num_queries 100 --logging_freq 40 --use_focal_cls --save_buffer --relabel --eval --epochs 50 --clip_r 1.0 --use_hyperbolic --unknown_weight 0.1 --hyperbolic_c 0.1 --hyperbolic_temp 0.2 --samples_per_category 1 --hyperbolic_coeff 0.05 --checkpoint_period 10 --start_relabelling 0 --emb_per_class 10 --all_background --empty_weight 0.1 --use_max_uperbound --family_regularizer --family_hyperbolic_coeff 0.02
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'HIERARCHICAL': ('bicycle', 'car', 'traffic light', 'fire hydrant', 'bird', 'cat', 'dog', 'backpack', 'frisbee', 'skis', 'bottle', 'wine glass', 'banana', 'apple', 'chair', 'sofa', 'tvmonitor', 'microwave', 'oven', 'book', 'person', 'motorbike', 'aeroplane', 'stop sign', 'horse', 'sheep', 'umbrella', 'snowboard', 'sports ball', 'cup', 'sandwich', 'orange', 'broccoli', 'pottedplant', 'bed', 'laptop', 'mouse', 'toaster', 'clock', 'vase', 'bus', 'train', 'parking meter', 'cow', 'elephant', 'bear', 'handbag', 'kite', 'baseball bat', 'baseball glove', 'fork', 'knife', 'carrot', 'hot dog', 'diningtable', 'remote', 'keyboard', 'sink', 'scissors', 'teddy bear', 'truck', 'boat', 'bench', 'zebra', 'giraffe', 'tie', 'suitcase', 'skateboard', 'surfboard', 'tennis racket', 'spoon', 'bowl', 'pizza', 'donut', 'cake', 'toilet', 'cell phone', 'refrigerator', 'hair drier', 'toothbrush', 'unknown'), 'BURST_VAL': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'person', 'motorbike', 'aeroplane', 'stop sign', 'horse', 'sheep', 'umbrella', 'snowboard', 'sports ball', 'cup', 'sandwich', 'orange', 'broccoli', 'pottedplant', 'bed', 'laptop', 'mouse', 'toaster', 'clock', 'vase', 'bus', 'train', 'parking meter', 'cow', 'elephant', 'bear', 'handbag', 'kite', 'baseball bat', 'baseball glove', 'fork', 'knife', 'carrot', 'hot dog', 'diningtable', 'remote', 'keyboard', 'sink', 'scissors', 'teddy bear', 'truck', 'boat', 'bench', 'zebra', 'giraffe', 'tie', 'suitcase', 'skateboard', 'surfboard', 'tennis racket', 'spoon', 'bowl', 'pizza', 'donut', 'cake', 'toilet', 'cell phone', 'refrigerator', 'hair drier', 'toothbrush', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
Not using distributed mode
git:
  sha: ea93b7477ef452cef37abcc97d935ecbb4406b82, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=10, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=80, CUR_INTRODUCED_CLS=0, unmatched_boxes=True, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=True, bbox_thresh=0.3, pretrain='exps/hypow_t4_ft_hierarchical_split.pth', nc_loss_coef=2, train_set='', test_set='burst_val_test', num_classes=81, nc_epoch=9, dataset='HIERARCHICAL', data_root='./data/OWOD', unk_conf_w=1.0, model_type='hypow', wandb_name='PROB_V1_t1', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, debug_epoch=False, debug_eval=False, use_2007=1, checkpoint_period=10, emb_per_class=10, momentum=0.1, logging_freq=40, eval_during_training=500, start_relabelling=0, all_background=True, relabel=True, ablation=False, layer_option=0, double_eval=False, use_hyperbolic=True, hyperbolic_coeff=0.05, family_hyperbolic_coeff=0.02, samples_per_category=1, hyperbolic_temp=0.2, hyperbolic_c=0.1, clip_r=1.0, update_freq=50, family_regularizer=True, save_objectness=False, hyperbolic_mean=False, unknown_weight=0.1, use_focal_cls=True, empty_weight=0.1, ablation_save_objectness=False, ablation_save_embedding=False, ablation_cosine=False, use_max_uperbound=True, collect_buffer=False, save_buffer=True, load_buffer=False, buffer_dir='', relevant_matrix_dir='', save_eval_det_file='', predict_custom=True, burst_subdataset='ArgoVerse', burst_subset='val', video_id=None, detections_path='/home/uig93971/src/Hyp-OW-burst-eval/detections', burst_annot_path='/home/uig93971/src/data/TAO/burst_annotations', tao_frames_path='/home/uig93971/src/data/TAO/frames', distributed=False)
Invalid class range: []
DINO resnet50
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (manifold): PoincareBall manifold
  (tpc): ToPoincare(c=0.1, train_x=False)
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
HIERARCHICAL

burst_val_test
None
Dataset OWDetection
    Number of datapoints: 13931
    Root location: ./data/OWOD
    [['val'], Compose(
    <datasets.transforms.RandomResize object at 0x7ff88566e470>
    Compose(
    <datasets.transforms.ToTensor object at 0x7ff8857954e0>
    <datasets.transforms.Normalize object at 0x7ff885796650>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
Running inference on BURST dataset
/home/uig93971/src/Hyp-OW-burst-eval/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/uig93971/src/Hyp-OW-burst-eval/models/hypow_deformable_detr.py:1176: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Processed videos 1/74
Time passed: 100.1511 seconds, Avg time per video: 100.1511 seconds
Processed videos 2/74
Time passed: 196.9727 seconds, Avg time per video: 98.4864 seconds
Processed videos 3/74
Time passed: 292.4313 seconds, Avg time per video: 97.4771 seconds
Processed videos 4/74
Time passed: 482.2761 seconds, Avg time per video: 120.5690 seconds
Processed videos 5/74
Time passed: 577.4848 seconds, Avg time per video: 115.4970 seconds
Processed videos 6/74
Time passed: 673.0797 seconds, Avg time per video: 112.1799 seconds
Processed videos 7/74
Time passed: 761.8911 seconds, Avg time per video: 108.8416 seconds
Processed videos 8/74
Time passed: 945.8580 seconds, Avg time per video: 118.2322 seconds
Processed videos 9/74
Time passed: 1041.0530 seconds, Avg time per video: 115.6726 seconds
Processed videos 10/74
Time passed: 1230.3857 seconds, Avg time per video: 123.0386 seconds
Processed videos 11/74
Time passed: 1413.3776 seconds, Avg time per video: 128.4889 seconds
Processed videos 12/74
Time passed: 1590.3650 seconds, Avg time per video: 132.5304 seconds
Processed videos 13/74
Time passed: 1685.5093 seconds, Avg time per video: 129.6546 seconds
Processed videos 14/74
Time passed: 1780.8759 seconds, Avg time per video: 127.2054 seconds
Processed videos 15/74
Time passed: 1876.0898 seconds, Avg time per video: 125.0727 seconds
Processed videos 16/74
Time passed: 2066.4791 seconds, Avg time per video: 129.1549 seconds
Processed videos 17/74
Time passed: 2199.5769 seconds, Avg time per video: 129.3869 seconds
Processed videos 18/74
Time passed: 2381.9352 seconds, Avg time per video: 132.3297 seconds
Processed videos 19/74
Time passed: 2477.1072 seconds, Avg time per video: 130.3741 seconds
Processed videos 20/74
Time passed: 2572.1282 seconds, Avg time per video: 128.6064 seconds
Processed videos 21/74
Time passed: 2762.2433 seconds, Avg time per video: 131.5354 seconds
Processed videos 22/74
Time passed: 2857.3475 seconds, Avg time per video: 129.8794 seconds
Processed videos 23/74
Time passed: 3047.8688 seconds, Avg time per video: 132.5160 seconds
Processed videos 24/74
Time passed: 3142.7412 seconds, Avg time per video: 130.9476 seconds
Processed videos 25/74
Time passed: 3237.9955 seconds, Avg time per video: 129.5198 seconds
Processed videos 26/74
Time passed: 3333.1524 seconds, Avg time per video: 128.1982 seconds
Processed videos 27/74
Time passed: 3428.4623 seconds, Avg time per video: 126.9801 seconds
Processed videos 28/74
Time passed: 3619.0037 seconds, Avg time per video: 129.2501 seconds
Processed videos 29/74
Time passed: 3758.9344 seconds, Avg time per video: 129.6184 seconds
Processed videos 30/74
Time passed: 3949.6067 seconds, Avg time per video: 131.6536 seconds
Processed videos 31/74
Time passed: 4044.4377 seconds, Avg time per video: 130.4657 seconds
Processed videos 32/74
Time passed: 4228.9277 seconds, Avg time per video: 132.1540 seconds
Processed videos 33/74
Time passed: 4323.6838 seconds, Avg time per video: 131.0207 seconds
Processed videos 34/74
Time passed: 4418.7363 seconds, Avg time per video: 129.9628 seconds
Processed videos 35/74
Time passed: 4507.5614 seconds, Avg time per video: 128.7875 seconds
Processed videos 36/74
Time passed: 4602.8100 seconds, Avg time per video: 127.8558 seconds
Processed videos 37/74
Time passed: 4697.4512 seconds, Avg time per video: 126.9581 seconds
Processed videos 38/74
Time passed: 4792.2200 seconds, Avg time per video: 126.1111 seconds
Processed videos 39/74
Time passed: 4886.9274 seconds, Avg time per video: 125.3058 seconds
Processed videos 40/74
Time passed: 4981.9051 seconds, Avg time per video: 124.5476 seconds
Processed videos 41/74
Time passed: 5076.6692 seconds, Avg time per video: 123.8212 seconds
Processed videos 42/74
Time passed: 5172.2391 seconds, Avg time per video: 123.1486 seconds
Processed videos 43/74
Time passed: 5361.1661 seconds, Avg time per video: 124.6783 seconds
Processed videos 44/74
Time passed: 5456.2959 seconds, Avg time per video: 124.0067 seconds
Processed videos 45/74
Time passed: 5639.8321 seconds, Avg time per video: 125.3296 seconds
Processed videos 46/74
Time passed: 5773.2632 seconds, Avg time per video: 125.5057 seconds
Processed videos 47/74
Time passed: 5962.9475 seconds, Avg time per video: 126.8712 seconds
Processed videos 48/74
Time passed: 6057.7582 seconds, Avg time per video: 126.2033 seconds
Processed videos 49/74
Time passed: 6197.5484 seconds, Avg time per video: 126.4806 seconds
Processed videos 50/74
Time passed: 6292.5132 seconds, Avg time per video: 125.8503 seconds
Processed videos 51/74
Time passed: 6482.1379 seconds, Avg time per video: 127.1007 seconds
Processed videos 52/74
Time passed: 6670.9036 seconds, Avg time per video: 128.2866 seconds
Processed videos 53/74
Time passed: 6765.7819 seconds, Avg time per video: 127.6563 seconds
Processed videos 54/74
Time passed: 6860.5042 seconds, Avg time per video: 127.0464 seconds
Processed videos 55/74
Time passed: 6955.5288 seconds, Avg time per video: 126.4642 seconds
Processed videos 56/74
Time passed: 7044.0004 seconds, Avg time per video: 125.7857 seconds
Processed videos 57/74
Time passed: 7139.3222 seconds, Avg time per video: 125.2513 seconds
Processed videos 58/74
Time passed: 7234.2276 seconds, Avg time per video: 124.7281 seconds
Processed videos 59/74
Time passed: 7423.7197 seconds, Avg time per video: 125.8258 seconds
Processed videos 60/74
Time passed: 7606.6342 seconds, Avg time per video: 126.7772 seconds
Processed videos 61/74
Time passed: 7701.6625 seconds, Avg time per video: 126.2568 seconds
Processed videos 62/74
Time passed: 7892.2210 seconds, Avg time per video: 127.2939 seconds
Processed videos 63/74
Time passed: 7987.4970 seconds, Avg time per video: 126.7857 seconds
Processed videos 64/74
Time passed: 8082.6446 seconds, Avg time per video: 126.2913 seconds
Processed videos 65/74
Time passed: 8177.2227 seconds, Avg time per video: 125.8034 seconds
Processed videos 66/74
Time passed: 8272.3070 seconds, Avg time per video: 125.3380 seconds
Processed videos 67/74
Time passed: 8361.4406 seconds, Avg time per video: 124.7976 seconds
Processed videos 68/74
Time passed: 8456.3088 seconds, Avg time per video: 124.3575 seconds
Processed videos 69/74
Time passed: 8640.4257 seconds, Avg time per video: 125.2236 seconds
Processed videos 70/74
Time passed: 8734.9758 seconds, Avg time per video: 124.7854 seconds
Processed videos 71/74
Time passed: 8924.4510 seconds, Avg time per video: 125.6965 seconds
Processed videos 72/74
Time passed: 9109.0375 seconds, Avg time per video: 126.5144 seconds
Processed videos 73/74
Time passed: 9285.3514 seconds, Avg time per video: 127.1966 seconds
Processed videos 74/74
Time passed: 9380.2265 seconds, Avg time per video: 126.7598 seconds
+ PY_ARGS=
+ python -u main_open_world.py --predict_custom --burst_subdataset YFCC100M --burst_subset val --detections_path /home/uig93971/src/Hyp-OW-burst-eval/detections --burst_annot_path /home/uig93971/src/data/TAO/burst_annotations --tao_frames_path /home/uig93971/src/data/TAO/frames --dataset HIERARCHICAL --PREV_INTRODUCED_CLS 80 --CUR_INTRODUCED_CLS 0 --train_set '' --test_set burst_val_test --epochs 191 --lr_drop 35 --model_type hypow --obj_loss_coef 8e-4 --obj_temp 1.3 --pretrain exps/hypow_t4_ft_hierarchical_split.pth --eval --wandb_project '' --wandb_name PROB_V1_t1 --wandb_project '' --lr_drop 40 --num_queries 100 --logging_freq 40 --use_focal_cls --save_buffer --relabel --eval --epochs 50 --clip_r 1.0 --use_hyperbolic --unknown_weight 0.1 --hyperbolic_c 0.1 --hyperbolic_temp 0.2 --samples_per_category 1 --hyperbolic_coeff 0.05 --checkpoint_period 10 --start_relabelling 0 --emb_per_class 10 --all_background --empty_weight 0.1 --use_max_uperbound --family_regularizer --family_hyperbolic_coeff 0.02
{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'HIERARCHICAL': ('bicycle', 'car', 'traffic light', 'fire hydrant', 'bird', 'cat', 'dog', 'backpack', 'frisbee', 'skis', 'bottle', 'wine glass', 'banana', 'apple', 'chair', 'sofa', 'tvmonitor', 'microwave', 'oven', 'book', 'person', 'motorbike', 'aeroplane', 'stop sign', 'horse', 'sheep', 'umbrella', 'snowboard', 'sports ball', 'cup', 'sandwich', 'orange', 'broccoli', 'pottedplant', 'bed', 'laptop', 'mouse', 'toaster', 'clock', 'vase', 'bus', 'train', 'parking meter', 'cow', 'elephant', 'bear', 'handbag', 'kite', 'baseball bat', 'baseball glove', 'fork', 'knife', 'carrot', 'hot dog', 'diningtable', 'remote', 'keyboard', 'sink', 'scissors', 'teddy bear', 'truck', 'boat', 'bench', 'zebra', 'giraffe', 'tie', 'suitcase', 'skateboard', 'surfboard', 'tennis racket', 'spoon', 'bowl', 'pizza', 'donut', 'cake', 'toilet', 'cell phone', 'refrigerator', 'hair drier', 'toothbrush', 'unknown'), 'BURST_VAL': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'person', 'motorbike', 'aeroplane', 'stop sign', 'horse', 'sheep', 'umbrella', 'snowboard', 'sports ball', 'cup', 'sandwich', 'orange', 'broccoli', 'pottedplant', 'bed', 'laptop', 'mouse', 'toaster', 'clock', 'vase', 'bus', 'train', 'parking meter', 'cow', 'elephant', 'bear', 'handbag', 'kite', 'baseball bat', 'baseball glove', 'fork', 'knife', 'carrot', 'hot dog', 'diningtable', 'remote', 'keyboard', 'sink', 'scissors', 'teddy bear', 'truck', 'boat', 'bench', 'zebra', 'giraffe', 'tie', 'suitcase', 'skateboard', 'surfboard', 'tennis racket', 'spoon', 'bowl', 'pizza', 'donut', 'cake', 'toilet', 'cell phone', 'refrigerator', 'hair drier', 'toothbrush', 'unknown')}
('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')
Not using distributed mode
git:
  sha: ea93b7477ef452cef37abcc97d935ecbb4406b82, status: has uncommited changes, branch: main

Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=10, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=80, CUR_INTRODUCED_CLS=0, unmatched_boxes=True, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=True, bbox_thresh=0.3, pretrain='exps/hypow_t4_ft_hierarchical_split.pth', nc_loss_coef=2, train_set='', test_set='burst_val_test', num_classes=81, nc_epoch=9, dataset='HIERARCHICAL', data_root='./data/OWOD', unk_conf_w=1.0, model_type='hypow', wandb_name='PROB_V1_t1', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, debug_epoch=False, debug_eval=False, use_2007=1, checkpoint_period=10, emb_per_class=10, momentum=0.1, logging_freq=40, eval_during_training=500, start_relabelling=0, all_background=True, relabel=True, ablation=False, layer_option=0, double_eval=False, use_hyperbolic=True, hyperbolic_coeff=0.05, family_hyperbolic_coeff=0.02, samples_per_category=1, hyperbolic_temp=0.2, hyperbolic_c=0.1, clip_r=1.0, update_freq=50, family_regularizer=True, save_objectness=False, hyperbolic_mean=False, unknown_weight=0.1, use_focal_cls=True, empty_weight=0.1, ablation_save_objectness=False, ablation_save_embedding=False, ablation_cosine=False, use_max_uperbound=True, collect_buffer=False, save_buffer=True, load_buffer=False, buffer_dir='', relevant_matrix_dir='', save_eval_det_file='', predict_custom=True, burst_subdataset='YFCC100M', burst_subset='val', video_id=None, detections_path='/home/uig93971/src/Hyp-OW-burst-eval/detections', burst_annot_path='/home/uig93971/src/data/TAO/burst_annotations', tao_frames_path='/home/uig93971/src/data/TAO/frames', distributed=False)
Invalid class range: []
DINO resnet50
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
running with exemplar_replay_selection
DeformableDETR(
  (transformer): DeformableTransformer(
    (encoder): DeformableTransformerEncoder(
      (layers): ModuleList(
        (0): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerEncoderLayer(
          (self_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout2): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): DeformableTransformerDecoder(
      (layers): ModuleList(
        (0): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): DeformableTransformerDecoderLayer(
          (cross_attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.1, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (reference_points): Linear(in_features=256, out_features=2, bias=True)
  )
  (manifold): PoincareBall manifold
  (tpc): ToPoincare(c=0.1, train_x=False)
  (class_embed): ModuleList(
    (0): Linear(in_features=256, out_features=81, bias=True)
    (1): Linear(in_features=256, out_features=81, bias=True)
    (2): Linear(in_features=256, out_features=81, bias=True)
    (3): Linear(in_features=256, out_features=81, bias=True)
    (4): Linear(in_features=256, out_features=81, bias=True)
    (5): Linear(in_features=256, out_features=81, bias=True)
  )
  (bbox_embed): ModuleList(
    (0): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (1): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (2): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (3): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (4): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (5): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (query_embed): Embedding(100, 512)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): Backbone(
      (body): IntermediateLayerGetter(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
)
number of params: 39742295
HIERARCHICAL

burst_val_test
None
Dataset OWDetection
    Number of datapoints: 13931
    Root location: ./data/OWOD
    [['val'], Compose(
    <datasets.transforms.RandomResize object at 0x7fed4d176470>
    Compose(
    <datasets.transforms.ToTensor object at 0x7fed4d2994e0>
    <datasets.transforms.Normalize object at 0x7fed4d29a650>
)
)]
Initialized from the pre-training model
<All keys matched successfully>
Running inference on BURST dataset
/home/uig93971/src/Hyp-OW-burst-eval/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
/home/uig93971/miniconda3/envs/hypow/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/uig93971/src/Hyp-OW-burst-eval/models/hypow_deformable_detr.py:1176: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_boxes = topk_indexes // out_logits.shape[2]
Processed videos 1/107
Time passed: 187.2713 seconds, Avg time per video: 187.2713 seconds
Processed videos 2/107
Time passed: 372.8454 seconds, Avg time per video: 186.4227 seconds
Processed videos 3/107
Time passed: 528.4133 seconds, Avg time per video: 176.1378 seconds
Processed videos 4/107
Time passed: 714.7011 seconds, Avg time per video: 178.6753 seconds
Processed videos 5/107
Time passed: 938.1460 seconds, Avg time per video: 187.6292 seconds
Processed videos 6/107
Time passed: 1093.2984 seconds, Avg time per video: 182.2164 seconds
Processed videos 7/107
Time passed: 1280.3138 seconds, Avg time per video: 182.9020 seconds
Processed videos 8/107
Time passed: 1466.6360 seconds, Avg time per video: 183.3295 seconds
Processed videos 9/107
Time passed: 1652.7035 seconds, Avg time per video: 183.6337 seconds
Processed videos 10/107
Time passed: 1839.4132 seconds, Avg time per video: 183.9413 seconds
Processed videos 11/107
Time passed: 1988.6202 seconds, Avg time per video: 180.7837 seconds
Processed videos 12/107
Time passed: 2174.6068 seconds, Avg time per video: 181.2172 seconds
Processed videos 13/107
Time passed: 2360.6717 seconds, Avg time per video: 181.5901 seconds
Processed videos 14/107
Time passed: 2510.1726 seconds, Avg time per video: 179.2980 seconds
Processed videos 15/107
Time passed: 2696.3180 seconds, Avg time per video: 179.7545 seconds
Processed videos 16/107
Time passed: 2882.6237 seconds, Avg time per video: 180.1640 seconds
Processed videos 17/107
Time passed: 3030.8835 seconds, Avg time per video: 178.2873 seconds
Processed videos 18/107
Time passed: 3217.8898 seconds, Avg time per video: 178.7717 seconds
Processed videos 19/107
Time passed: 3441.2644 seconds, Avg time per video: 181.1192 seconds
Processed videos 20/107
Time passed: 3666.4662 seconds, Avg time per video: 183.3233 seconds
Processed videos 21/107
Time passed: 3843.9803 seconds, Avg time per video: 183.0467 seconds
Processed videos 22/107
Time passed: 4030.1404 seconds, Avg time per video: 183.1882 seconds
Processed videos 23/107
Time passed: 4215.4475 seconds, Avg time per video: 183.2803 seconds
Processed videos 24/107
Time passed: 4401.2794 seconds, Avg time per video: 183.3866 seconds
Processed videos 25/107
Time passed: 4588.0410 seconds, Avg time per video: 183.5216 seconds
Processed videos 26/107
Time passed: 4760.3369 seconds, Avg time per video: 183.0899 seconds
Processed videos 27/107
Time passed: 4914.0601 seconds, Avg time per video: 182.0022 seconds
Processed videos 28/107
Time passed: 5100.7603 seconds, Avg time per video: 182.1700 seconds
Processed videos 29/107
Time passed: 5286.8527 seconds, Avg time per video: 182.3053 seconds
Processed videos 30/107
Time passed: 5473.5912 seconds, Avg time per video: 182.4530 seconds
Processed videos 31/107
Time passed: 5660.0283 seconds, Avg time per video: 182.5816 seconds
Processed videos 32/107
Time passed: 5817.8981 seconds, Avg time per video: 181.8093 seconds
Processed videos 33/107
Time passed: 6000.4402 seconds, Avg time per video: 181.8315 seconds
Processed videos 34/107
Time passed: 6186.3915 seconds, Avg time per video: 181.9527 seconds
Processed videos 35/107
Time passed: 6372.6929 seconds, Avg time per video: 182.0769 seconds
Processed videos 36/107
Time passed: 6559.2712 seconds, Avg time per video: 182.2020 seconds
Processed videos 37/107
Time passed: 6652.6308 seconds, Avg time per video: 179.8008 seconds
Processed videos 38/107
Time passed: 6840.2999 seconds, Avg time per video: 180.0079 seconds
Processed videos 39/107
Time passed: 7026.2206 seconds, Avg time per video: 180.1595 seconds
Processed videos 40/107
Time passed: 7213.4736 seconds, Avg time per video: 180.3368 seconds
Processed videos 41/107
Time passed: 7437.1546 seconds, Avg time per video: 181.3940 seconds
Processed videos 42/107
Time passed: 7624.1660 seconds, Avg time per video: 181.5278 seconds
Processed videos 43/107
Time passed: 7810.0022 seconds, Avg time per video: 181.6280 seconds
Processed videos 44/107
Time passed: 7995.6580 seconds, Avg time per video: 181.7195 seconds
Processed videos 45/107
Time passed: 8088.5867 seconds, Avg time per video: 179.7464 seconds
Processed videos 46/107
Time passed: 8274.5586 seconds, Avg time per video: 179.8817 seconds
Processed videos 47/107
Time passed: 8460.6060 seconds, Avg time per video: 180.0129 seconds
Processed videos 48/107
Time passed: 8613.8154 seconds, Avg time per video: 179.4545 seconds
Processed videos 49/107
Time passed: 8799.9575 seconds, Avg time per video: 179.5910 seconds
Processed videos 50/107
Time passed: 8954.7225 seconds, Avg time per video: 179.0944 seconds
Processed videos 51/107
Time passed: 9141.3308 seconds, Avg time per video: 179.2418 seconds
Processed videos 52/107
Time passed: 9322.1794 seconds, Avg time per video: 179.2727 seconds
Processed videos 53/107
Time passed: 9500.0031 seconds, Avg time per video: 179.2453 seconds
Processed videos 54/107
Time passed: 9654.8478 seconds, Avg time per video: 178.7935 seconds
Processed videos 55/107
Time passed: 9810.4864 seconds, Avg time per video: 178.3725 seconds
Processed videos 56/107
Time passed: 9963.0393 seconds, Avg time per video: 177.9114 seconds
Processed videos 57/107
Time passed: 10149.5447 seconds, Avg time per video: 178.0622 seconds
Processed videos 58/107
Time passed: 10336.1335 seconds, Avg time per video: 178.2092 seconds
Processed videos 59/107
Time passed: 10522.6468 seconds, Avg time per video: 178.3499 seconds
Processed videos 60/107
Time passed: 10671.5181 seconds, Avg time per video: 177.8586 seconds
Processed videos 61/107
Time passed: 10801.2294 seconds, Avg time per video: 177.0693 seconds
Processed videos 62/107
Time passed: 10987.3514 seconds, Avg time per video: 177.2153 seconds
Processed videos 63/107
Time passed: 11172.3258 seconds, Avg time per video: 177.3385 seconds
Processed videos 64/107
Time passed: 11358.4330 seconds, Avg time per video: 177.4755 seconds
Processed videos 65/107
Time passed: 11507.6558 seconds, Avg time per video: 177.0409 seconds
Processed videos 66/107
Time passed: 11694.0119 seconds, Avg time per video: 177.1820 seconds
Processed videos 67/107
Time passed: 11880.3268 seconds, Avg time per video: 177.3183 seconds
Processed videos 68/107
Time passed: 12028.9173 seconds, Avg time per video: 176.8958 seconds
Processed videos 69/107
Time passed: 12177.3393 seconds, Avg time per video: 176.4832 seconds
Processed videos 70/107
Time passed: 12363.5515 seconds, Avg time per video: 176.6222 seconds
Processed videos 71/107
Time passed: 12549.5696 seconds, Avg time per video: 176.7545 seconds
Processed videos 72/107
Time passed: 12736.4055 seconds, Avg time per video: 176.8945 seconds
Processed videos 73/107
Time passed: 12894.6552 seconds, Avg time per video: 176.6391 seconds
Processed videos 74/107
Time passed: 13057.3481 seconds, Avg time per video: 176.4507 seconds
Processed videos 75/107
Time passed: 13224.3143 seconds, Avg time per video: 176.3242 seconds
Processed videos 76/107
Time passed: 13377.0889 seconds, Avg time per video: 176.0143 seconds
Processed videos 77/107
Time passed: 13563.1557 seconds, Avg time per video: 176.1449 seconds
Processed videos 78/107
Time passed: 13748.9641 seconds, Avg time per video: 176.2688 seconds
Processed videos 79/107
Time passed: 13935.0014 seconds, Avg time per video: 176.3924 seconds
Processed videos 80/107
Time passed: 14123.3012 seconds, Avg time per video: 176.5413 seconds
Processed videos 81/107
Time passed: 14310.3687 seconds, Avg time per video: 176.6712 seconds
Processed videos 82/107
Time passed: 14497.0848 seconds, Avg time per video: 176.7937 seconds
Processed videos 83/107
Time passed: 14683.3973 seconds, Avg time per video: 176.9084 seconds
Processed videos 84/107
Time passed: 14831.9904 seconds, Avg time per video: 176.5713 seconds
Processed videos 85/107
Time passed: 15017.5418 seconds, Avg time per video: 176.6770 seconds
Processed videos 86/107
Time passed: 15092.0373 seconds, Avg time per video: 175.4888 seconds
Processed videos 87/107
Time passed: 15278.5565 seconds, Avg time per video: 175.6156 seconds
Processed videos 88/107
Time passed: 15465.4127 seconds, Avg time per video: 175.7433 seconds
Processed videos 89/107
Time passed: 15614.4986 seconds, Avg time per video: 175.4438 seconds
Processed videos 90/107
Time passed: 15800.6989 seconds, Avg time per video: 175.5633 seconds
Processed videos 91/107
Time passed: 15987.5007 seconds, Avg time per video: 175.6868 seconds
Processed videos 92/107
Time passed: 16059.1143 seconds, Avg time per video: 174.5556 seconds
Processed videos 93/107
Time passed: 16246.9077 seconds, Avg time per video: 174.6979 seconds
Processed videos 94/107
Time passed: 16402.6286 seconds, Avg time per video: 174.4960 seconds
Processed videos 95/107
Time passed: 16589.0489 seconds, Avg time per video: 174.6216 seconds
Processed videos 96/107
Time passed: 16775.2353 seconds, Avg time per video: 174.7420 seconds
Processed videos 97/107
Time passed: 16961.6140 seconds, Avg time per video: 174.8620 seconds
Processed videos 98/107
Time passed: 17147.2411 seconds, Avg time per video: 174.9718 seconds
Processed videos 99/107
Time passed: 17333.3375 seconds, Avg time per video: 175.0842 seconds
Processed videos 100/107
Time passed: 17519.1812 seconds, Avg time per video: 175.1918 seconds
Processed videos 101/107
Time passed: 17674.0614 seconds, Avg time per video: 174.9907 seconds
Processed videos 102/107
Time passed: 17817.4604 seconds, Avg time per video: 174.6810 seconds
Processed videos 103/107
Time passed: 17972.2541 seconds, Avg time per video: 174.4879 seconds
Processed videos 104/107
Time passed: 18158.2841 seconds, Avg time per video: 174.5989 seconds
Processed videos 105/107
Time passed: 18344.4480 seconds, Avg time per video: 174.7090 seconds
Processed videos 106/107
Time passed: 18506.6018 seconds, Avg time per video: 174.5906 seconds
Processed videos 107/107
Time passed: 18661.3676 seconds, Avg time per video: 174.4053 seconds
